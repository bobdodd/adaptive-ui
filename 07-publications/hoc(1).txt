Title:
DO YOU HEAR WHAT I SEE? THE MULTI-SENSORY DESIGN SPACE IN ACCESSIBLE MOBILE DEVICES
Presenters & Contributors:
R. J. Dodd
Address:
Special Needs Computing Research Unit, School of Computing, University of Teesside, Middlesbrough, Tees Valley TS1 3BA

The current generation of hand-held mobile devices are not particularly accessible to disabled users. This is partly a systemic ergonomic issue, and partly an issue with how feedback is provided from the device to the user. For example, a typical mobile phone provides only a small visual interface and has limited sound and tactile feedback capabilities. This work addresses the problem by attending to the process through which user interfaces for mobile devices are generated, delaying as late as possible the selection of appropriate presentation metaphors in the construction of the concrete user interface, with the project aim of maximising the number of potential design spaces (visual, sonic, haptic, ergonomic and conceptual) that are selectable by the user of the device, rather than being fixed by the designer. To achieve this aim, the work revisits Nesbitt’s ‘Multi-sensory Design Space’, and the ‘Amsterdam Model’ of multimedia to define a method for automated translation of abstract user interfaces into concrete implementations through a semantic decomposition of each design space. In practise this will result in user capabilities and preferences being mapped to presentation metaphors, presentation metaphors to design spaces, and design spaces to device capabilities.  In doing so, the work extends and tailors Mellor’s ‘Executable And Translatable UML’ method for software implementation through the translation of abstract models. In order to test and demonstrate the developed method, the work addresses two specific design spaces: visual and sonic, mapping the visual metaphors of a mobile phone into the sonic design space, allowing users to select an interface style appropriate to their capabilities and their preferences on a single device. Do you hear what I see? That depends on which presentation metaphors and design spaces that you, as a user, choose to select on your device.

