Accessibility and Model-View-Controller

     Robert Dodd
University of Teesside School of Computing Tees Valley TS1 3BA, UK +44 (0)1642 342656
r.dodd@tees.ac.uk

Dr. Steve Green
University of Teesside School of Computing Tees Valley TS1 3BA, UK +44 (0)1642 342656
s.j.green@tees.ac.uk
 Dr. Elaine Pearson
University of Teesside School of Computing Tees Valley TS1 3BA, UK +44 (0)1642 342656
e.pearson@tees.ac.uk 

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.
W4A2007, May 07–08, 2007, Banff, Canada. Co-Located with the 16th 
International World Wide Web Conference. 
Copyright 2007 ACM 1-59593-590-8/06/0010 ...$5.00. 


ABSTRACT
A recurring theme in accessible user interface design is adaptation; adaptation in terms of changes to the physical characteristics of the computing devices involved, and adaptation in terms of changes to the style of modalities used to communicate with the user. 
Categories and Subject Descriptors
D.2.2 [Design Tools and Techniques]: user interfaces. K.4.2 [Social Issues]: Assistive Technologies for persons with disabilities. 
General Terms
Measurement, Design, Human Factors.
Keywords
Abstract User Interface, Hypertext, Hypermedia, Dexter Model.
INTRODUCTION
The World Wide Web as it exists today is a collection of electronic documents that reference each other using a particular model of navigation that is a subset of the Dexter Model of Hypertext [1]. The layout of content within a document, and the content itself, may change, but those changes do not affect the links between the documents. Documents in this sense are Web pages, not websites, and this is where the problems for assistive technology begin; Web pages are pre-defined “chunks” of the whole website, with the chunks selected for some unexplained (to the web browser) logical grouping, screen size, or user cognitive load. If content must adapt for a user’s physical or cognitive capabilities, perhaps migrating from the visual to the sonic design space, it is very difficult to ”de-chunk” back to the original semantic model of content, in order to re-group and re-organize the content; screen reading applications such as Jaws [2] do not even attempt to do so.
Some semantic information within a Web page is available to assistive technology, and with the latest HTML 5 draft [3], the quantity of explicit semantic content is growing.  However, that semantic information is provided by a very limited and stylized set of document tags that describe a document in terms of heading, paragraphs, images, and tabular content. Further, a limited number of interaction modalities are also supported from which semantic inferences may be drawn. In practice however, semantic tags are misused to describe document layout, and scripting, particularly AJAX [4] scripting, is used to create new interaction modalities that are largely opaque to assistive technology. Keyword completion in Google Suggest [5], where a drop-down window appears and populates with possible completions for the search string being entered, is an example of these new modalities; this research has shown screen reader applications to become either mute to the offered options, or not notice subsequent updates to the presented options after first display. 
The first step taken in approaching these problems was to produce a general model of a document as a whole, for example a website, in terms of its content, semantics, and navigation, and then to consider what it means to adapt such content.
THE DEXTER MODEL
The Dexter Model of Hypertext [1] is a synthesis of the capabilities of existing hypertext models in 1988; no single hypertext system supported the entire feature set. The Dexter Model was conceived as a layered model of content navigation with a runtime layer that depended upon a storage layer, which delegated document structure largely to individual components in a within-component layer. The Dexter Model was further extended to support synchronized Hypermedia, that extension is known as the Amsterdam Model. As part of this research, a simplified, semantically based description of the Dexter Model was created and is shown in  REF _Ref64736596 Figure 1. The additional hypermedia elements are shown emboldened.

Figure  SEQ Figure \* ARABIC 1 – Dexter/Amsterdam Model
 REF _Ref64736596 Figure 1 shows how Components are related to each other, with a Component being a document, or part of a document. A Component may decompose into many smaller components, or it may be a Link Component between two or more Components (i.e. a hyperlink).  For any one Component, there may be many Presentation Specifications, which provide hints to the Runtime Layer as to how to express the Component. The Dexter Model does not describe the content or format of such hints, but rather identifies their capability. Components are further characterized by Attributes that describe semantic notions about the Component that are of use to the Runtime Layer. Again the format of the attributes, and potential values is left to specific implementations. Link Components within the Dexter Model are not required to be constant; they are required only to be known when navigated. This allows for adaptive environments where navigation is rule-based, for example where a component is a search in Google; that there may be search results is certain, but the actual number depends upon the search terms given. This feature is not directly supported in HTML except by use of web page scripting.
AMSTERDAM MODEL
The Amsterdam Model of Hypermedia [6] is an extension of the Dexter Model of Hypertext that brings the concept of continuous time to hypertext, and is shown in  REF _Ref64736596 Figure 1 with the Amsterdam Model extensions emboldened. In Dexter, presentation of content is essentially static, with changes in presented Components occurring when a user navigates a link, resulting in an event-driven, discrete model of time. With the Amsterdam Model, video and audio streams are introduced as navigable, and synchronized content, so that for example, certain hyperlinks may be valid for particular scenes of a movie, or an audio track may begin playing after a specific time during a movie. The Amsterdam Model also attempts to handle competition for browser and/or scarce device resources, allowing synchronization and queuing for audio channels, video play-out, and potentially, screen real estate. This model of time and synchronization underpins the SMIL language [7] for synchronized multimedia on the web. SMIL itself is a component of the Multimedia Messaging System format (MMS) [8] used to send multimedia messages between mobile phones. SMIL, and the Amsterdam Model are to mobile phones what HTML and the Dexter Model are to web browsing, so any accessibility issues that impact the use of the Dexter Model also impact upon web browsing and the Amsterdam Model, and any accessibility issues with the Amsterdam Model impact upon both multimedia web browsing and the Multimedia Messaging System used in mobile phones. 
The Atom Component, which is plain text content in Dexter, is further classified into Text Component and Media Component Reference in Amsterdam in order to allow the model to refer to images, video, and audio held elsewhere (the Dexter Model is assumed to contain all the content that it presents). Synchronization between components is achieved through the use of Synchronization Arcs that describe timing relationships between components. Components are explicitly grouped into Channels for presentation. (Note that  REF _Ref64736596 Figure 1 is a somewhat simplified in this respect, and a more comprehensive description is given in [6]).
Accessibility issues exist around the semantic meaning of synchronized content in the Amsterdam Model; if a web browser does not know whether an audio stream is background music or spoken text, it becomes difficult to adapt multimedia content between design spaces, for example when adding additional text-to-speech. It is also important to understand why content is being synchronized. Taking the simple example of alert notification in a messaging application, notification of an incoming text message can be briefly delayed to allow, say, play-out of a text-to-speech message to complete; This requires an understanding of event priorities, requiring in turn semantic knowledge of the content. 
The five layer CISNA Document Model, created as part of this research, breaks the semantic view of the Amsterdam Model into five separate concerns: (i) raw content, (ii) core navigation between components, (iii) resolution of links between components, (iv) presentation of components, and (v) the semantic meaning of the content referred to as “within-component”. 
FIVE LAYER DOCUMENT MODEL
The original Dexter three-layer model, and the new CISNA (Content Inventory Semantics Navigation Adaptation) five-layer document model are shown for comparison in  REF _Ref64777253 Figure 2 and in  REF _Ref64776844 Figure 3.  
At the centre of the Dexter Model in  REF _Ref64777253 Figure 2 is the Storage Layer, which describes navigation between documents and between Components within a document. Description of semantic meaning is delegated to the Within-component Layer, which exposes Anchors (potential sources and destinations for navigating the documents) to the Storage Layer. The Storage Layer presents Components and potential navigation paths to the Runtime Layer, which is responsible for interacting with the user, and for resolving any “dangling links”, such as, say, Google’s search results, which are not known until a search has been executed.

Figure  SEQ Figure \* ARABIC 2 – Three-Layer Dexter Model
The five-layer CISNA model in  REF _Ref64776844 Figure 3 peels most of the Dexter Runtime Layer away to form separate Development and Runtime Systems that have visibility of all five layers (Development and Runtime is a distinction that exists within the detail of the Dexter Model itself). What remains of the Runtime Layer is link resolution, which forms part of the Adaptation Layer. Responsibility for rendering content to the user is explicitly moved out from the layers into the Runtime System, leaving an Abstract User Interface describing content and interaction. In

Figure  SEQ Figure \* ARABIC 3 – Five-layer CISNA Model
doing so, it follows the trend within web-content standards towards full separation of content from rendering, for example XForms [9]. What remains of the Dexter (and Amsterdam) Model is then considered within five independent abstractions, each relating to the immediately adjacent layers in the diagram. Note that the Adaptation Layer is considered adjacent to all other layers (expressed by the shaded arrowhead in  REF _Ref64776844 Figure 3). In terms of the level of abstraction from the tangible content of text and multimedia, abstraction builds from bottom-to-top within the diagram.
The lower four layers of the CISNA model are described below, with discussion of the Adaptation Layer is postponed until later.
Navigation Layer
The Navigation Layer of CISNA and the Storage Layer of the Dexter/Amsterdam models are closely related. The Navigation Layer, shown in  REF _Ref64806350 Figure 4, is primarily a collection of Nodes. A Node is something a user can navigate to or from, a Component in Dexter-speak. Nodes may also be collected form sub-groups of a Node (but not including itself), and this corresponds to the Composite Component of Dexter. Sub-groups exist primarily to support grouping of video, audio and captioning. It is not intended to describe “pages” (of documents or of websites), which are considered rather to be Views of content.


Figure  SEQ Figure \* ARABIC 4 - Navigation Layer

A View is a collection of navigable Nodes that are presented contemporaneously to the user. Presented content may not be visible or navigable to the user at all times, for example when content is presented visually in a scrollpane. Views are how the CISNA model supports the Dexter 1:M Link Components (represented by multiple Arcs for each Link Component). A View may also represent something else: a “chunk” of document, a web page, a chapter of a book, a volume of an encyclopedia; in fact any level of document decomposition relevant to presentation of content to the user. Views are not inherently hierarchical, although they can be, and the same Nodes may appear multiple times in different Views. The Dexter Model has just one View at this level of abstraction: the underlying content.
Navigation between Nodes is described by Edges, where an Edge is a navigable path between two Nodes. Edges correspond to individual Link Components in Dexter. There may be multiple Edges between the same two Nodes as we may navigate between them for different reasons: the “back” button and HTML fragment, <a href=”http://…”> can lead to the same Web page.
Both Nodes and Edges may have related Node or Edge Attributes. These attributes have a number of possible uses, one of which is timing synchronization. In a multimedia document for example, some edges are only available for navigation at particular times, and some nodes within a view may also be presented for a limited time. It is also possible to envision more complex synchronization issues; for example the visibility of certain Nodes in a View may be controlled by the Edge by which the user arrives at the View. This corresponds to use of Synchronization Arcs in the Amsterdam Model. Unlike the Amsterdam Model, detailed timing synchronization of multimedia elements, for example timing of captions on video, is assumed to take place in the lower layers of CISNA (unless the user can independently navigate to/from individual video/audio/caption elements.
Semantics Layer
The Semantics Layer, shown in  REF _Ref64806822 Figure 5, is a rule-based meta-model, similar in approach to Prolog [10]; Prolog has ‘facts’ and ‘rules’ whilst the Semantics Layer has ‘rules’ and ‘statements’. The Semantics Layer also has nouns, verbs, ontologies, and notions.

Figure  SEQ Figure \* ARABIC 5 - Semantics Layer
The idea behind this approach is to create a model of semantics that can express anything from a word processing document, to a website, to an interactive game, to an augmented reality system. Each of these “document types” has differing semantic rules on how information is organized, and yet all have general abstract rules and concrete statements that can be expressed by the nine-element model of  REF _Ref64806822 Figure 5.
Those nine elements group information according to Content Ontologies. Example ontologies include Container, Coordinate System, Media and Menu. For each ontology there exists a set of nouns that describe relevant entities within that ontology, for example Menu and Menu Item are both ‘nouns’ that describe entities within a Menu ontology. Similarly, Title, Protagonist, and Director may be ‘nouns’ in a Media ontology. Whilst verbs could also be considered part of the ontology, in this case they are deliberately left outside of the definition in order for the same conceptual meaning to apply across ontologies. Examples of Verbs in this context include: ‘is a’, ‘contains’, ‘scopes’, ‘expands upon’, and ‘follows’.
Nouns and verbs are used to make simple noun-verb-noun rules that describe underlying semantics. Simple example are; “Menu contains Menu Item”, “Menu Item follows Menu Item”, “Heading scopes Section”. These rules are the analogue of “facts” in Prolog.
One verb stands out from the rest: “is a”. A Menu is (sometimes) a List, a List Item is (sometimes) a List, and a Menu Item is (sometimes) a List Item. Logically therefore, if a List contains List Items, then a Menu contains Menu Items.  Consequently, the Semantics Layer is constructed around the concept of conditional multiple inheritance. Conditional multiple inheritance is chosen to allow easy expression of concepts such as a Menu is sometimes is list of items, and sometimes a grid of items, for example the top-level menu on the Apple iPhone.
To support the Semantics Layer, a simple graphical notation was developed to express inheritance relationships and rules. An example of its use is shown in  REF _Ref64808718 Figure 6.

Figure  SEQ Figure \* ARABIC 6 - Semantic Rules
In  REF _Ref64808718 Figure 6:
Rectangular boxes represent nouns.
Solid, unlabeled arrowed lines represent “A is a B”.
Dashed, arrowed lines represent “A is sometimes a B”.
Labeled arrowed lines represent all other noun-verb-noun associations; the text label represents the verb and its multiplicity.
Rounded boxes represent attributes of the related noun.
Rounded boxes with value lists represent the range of values for the related attribute.
Whilst ontologies play no active part in the notation above, they form an important role in the implementation of the CISNA Model; the Runtime System must understand the nouns contained within the ontologies in use in order to be able to render the content. So, in the same way XML [11] uses schemas and DTDs, CISNA use ontologies to identify the kind of semantic structures in use within the Semantics Layer, and the first task of the Runtime system is to validate that only supported ontologies are used by the document.
The notation itself is designed to be relatively simple to draw by hand, and easy to implement as a tool within the Development System. The small number of symbols and connectors make it a simple matter to create a validating editor application. No claims are made for the efficiency of the notation.
In the same way the Semantics Layer uses nouns, verbs, and rules to describe underlying semantic relationships, it also uses notions and statements to describe the concrete semantics of a particular document. A notion expresses some renderable concept, such as a specific menu, menu item, viewport, indicator (e.g. a message to a user); as such, notions are a specialization of nouns. In this way a Search Menu is a specialization of Menu, and Site Title is a specialization of Title. A statement links two notions together with a rule describing that relationship. The rule must be valid between both noun specializations in the statement. Statements are analogous to rules in Prolog.
The notation used to express rules was extended to support traceability from notion to noun and statement to rule. An example of its use is given in  REF _Ref64811133 Figure 7.

Figure  SEQ Figure \* ARABIC 7 - Semantics Statements
In  REF _Ref64811133 Figure 7:
Rectangular boxes may also represent notions.
Labeled arrowed lines may also represent statements.
Rounded boxes may also represent notion attributes.
Rounded boxes with a value represent notion attribute values.
Missing from  REF _Ref64811133 Figure 7 are the relationships between notion attributes and the noun attributes that they represent, these have been left off for clarity. 
One of the ideas behind the use of rules and statements in the Semantics Layer is the idea of having validatable models. Rules are common across many documents, statements describe specific content in a document; validating rules is analogous to validating an XML document using DTDs and Schemas.
For a Statement to be true, the Notions on each side of the rule to be applied must inherit from Nouns for which the rule is true. 
For an inheritance relationship to be true, a Notion must contain a Notion Attribute for each Noun Attribute in the inheritance tree, and for no others. Further, it must contain exactly one attribute, however many times the noun appears in the inheritance tree. This is needed to make the multiple inheritance work; we would not want two sets of attributes for Content (if it had any).
The chosen rules to validate non-inheritance statement ‘A rule B’ are:
‘A’ and ‘B’ must be notions.
‘A’ must inherit a noun on the left-hand side of the ‘rule’.
‘B’ must inherit a noun on the right-hand side of the ‘rule’.
The implied inheritance trees for ‘A’ and ‘B’ must be valid.
The multiplicity of the statement must be equal to, or stricter than the multiplicity of the referred-to ‘rule’.
The chosen rules to validate inheritance statement ‘A is a B’ are:
For each attribute noun between ‘A’ and ‘B’ there must exist a counterpart notion attribute in ‘A’.
Each counterpart notion attribute must be in the same attribute range as the noun attribute and must hold a valid value for that range.
There must be no duplicate notion attributes in ‘A’.
Any other ‘orphan’ notion attributes in ‘A’ must be counterpart attributes in another valid inheritance statement that involves a non-inheritance statement either from, or to, ‘A’.
Inventory Layer
The Inventory Layer, shown in  REF _Ref64814754 Figure 8, expresses all of the content available for interaction with a user. In terms of the Dexter/Amsterdam models, the Inventory Layer expresses the knowledge of content contained within a Component, and associates Anchors with individual inventory elements rather than with Components.
The primary goal of the Inventory Layer is to provide a buffer between content and semantic meaning by providing an indexing service. In doing so, it steps away from the Dexter Model by making all content storage external to the model This is a logical extension of the Amsterdam Model’s method of content referencing for multimedia. 
 

Figure  SEQ Figure \* ARABIC 8 - Inventory Layer
An example of usage is that of adding bullet points to a document. An image used for a bullet point may be used multiple times in the same document, and may appear additionally on a tool-bar of bullet styles for the user to choose from. This gives the same image two semantic meanings (a bullet point, and the style of a bullet point), and multiple instances of its use. The Inventory Layer records only the properties of the image such as size, resolution, and encoding format, plus a reference to the location of the content (including mirror locations if required). Semantic meaning is left to the Semantics Layer, and grouping of bullet points for presentation is left to the Navigation Layer.
A second example of usage is in interaction with tangible items. This occurs with augmented reality systems, and with tangible user interfaces such as the marble answer-phone [12]. In both cases there exists within the ‘document’ physical items whose position and properties have semantic meaning within the document, and user manipulation of them causes logical navigation between groups of content. With the marble answer-phone, each message recorded delivers one marble into a bowl; feeding a marble back into the machine causes that particular message to play. An augmented reality example would be a navigation aid for users with low vision, where visual object recognition is used to identify known items in the real world and report their status and location to the user using ear-cons. Both the marbles, and the real-world items are referenced as Media Elements. 
A third example of usage is concerned with text. The Inventory Layer provides a buffer between content and semantic meaning, but the nature of text is that, at the most basic level, it is a stream of symbols and punctuation bound together by semantics. In terms of the Inventory, it is necessary to consider how much text is referenced as a single Media Element. The working definition chosen for this Model is: 
A text Media Element comprises the largest block of text content possible that contains within it no semantic detail of interest to the Semantics Layer of the parent document.
Each Media Element is characterized by a Raw Media Type, with the range of types specific to the mark-up notation supporting the Model. Raw Media Types, particularly in adaptive systems, may not necessarily represent the format presented to a user. A simple example is text, where text may present visually using fonts, or as computer-generated speech, or as Braille; in terms of defining the construction of a user interface, it is the properties of the presented content that is of interest, not the original raw form. Tabular and graphical data may similarly have multiple forms of expression. For this reason, the CISNA Model expands on the Amsterdam Model's concept of media type, defining the Inventory Layer as containing  'formatted' elements in addition to raw content. A Formatted Media Element references the raw content, but it is the Presented Media Type that declares the final presented form, and the Format Attributes that define how this is to be achieved (e.g. text-to-speech using a specific voice). 
One of the less considered areas of hypermedia is transient content. Whilst the Dexter Model does provide a view of live content modification, the underlying assumption is that content is static unless users explicitly request modification. Consideration of transient content is of particular importance to any adaptive system or assistive technology that relies on transcoding content between design spaces, for example from visual to sonic. It is necessary to know what content is transient, when it is valid for presentation, and when it changes, in order to ensure that appropriate synchronization is provided where there is competition for resources such as with audio channels. It is also necessary to deal with the case where ear-cons must be added to notify changes in, say, textual content when content plays out in text-to-speech. Within CISNA, meaning and validity are modeled within the Semantics Layer, but the placeholders for such content also exist as Media Elements in the Inventory Layer. 
(External) Content Layer
Within CISNA, all content is considered external to any descriptions of its use and meaning. This is an extension of the Amsterdam Model’s concept of a Media Component Reference where all multimedia content is considered to be external to the model. CISNA’s separation of semantic structure from storage allows for re-use of text elements within a document, and for text element to be referred to in, say, external databases. 
The CISNA model places no restrictions on what may be considered content, so long as provision exists to adequately reference that content from the Inventory Layer. It may be, for example, images on the Web, text in an encyclopedia database, or physical items in an augmented reality environment.
BRIDGES AND COUNTERPARTS
The five-layer CISNA document model introduced in Section  REF _Ref64872844 \r 4, expresses the content and structure of a document as five independently modeled abstractions. The abstractions are considered as layers within a stack of increasing abstract concepts. 
Those layers that are adjacent to each other express semantically related concepts, and that collection of concepts is defined to be a bridge between the related layers. This approach is a variation on the Shlaer-Mellor [13] analysis method that describes an application as a hierarchy of dependent, but independently modeled problem domains, with the dependencies described by a bridge. In CISNA, there are no dependencies between the models/layers, but simply relationships that are expressed as models.

Figure  SEQ Figure \* ARABIC 9 - Inventory & Semantics Bridge

The Inventory & Semantics Bridge, shown in  REF _Ref64875430 Figure 9, expresses the three relationships that link the Inventory and Semantics layers: Element in Noun, Element in Notion, and Cue in Notion. The three relationships map media elements to the notions and nouns of the semantics layer. For example the text “Google Maps” may relate to notion “Site Title”. Many media elements may map to the same notion (for example to support multiple languages) and may be of different raw media types (for example strings of Rebus Symbols [14] to support users with specific learning difficulties). Where there are multiple alternative mappings, a reason code is provided for each mapping in the bridge.

Figure  SEQ Figure \* ARABIC 10 - Semantics & Navigation Bridge
The Semantics & Navigation Bridge, shown in  REF _Ref64889578 Figure 10, maps between Notions in the Semantics Layer and Nodes in the Navigation Layer. For example the notion, “Google Maps” may map to the View node, “Google Maps Application”, as may the notion, “Google Maps Description”. This occurs because a bridge describes counterparts, i.e.  “Notion A exists when we have Node B”, and not “Notion A is Node B”.
Adaptation Layer
The Content, Semantics, Inventory and Navigation Layers describe possible organizations of content, semantics and navigation within a document. What makes the document concrete is the specific selection of instances of each layer, where an instance is a transaction that expresses change to the layers in terms of add/modify/delete. Because an instance is a kind of transaction, the order of application of instances is important, hence the relationship between Instance and InstanceApplication, shown in  REF _Ref64859045 Figure 11. Example instances may be “Default User”, “Default to Low Vision User”, “Default User to Blind User”, “Blind User to Blind User with Restricted Mobility”.

Figure  SEQ Figure \* ARABIC 11 - Adaptation Layer
There are two types of document to consider during adaptation of content: passive and dynamic. A document is passive if no new instances are realized within the document during a single session with a user.  A newspaper is passive because once printed, no further modification is made. It may be pulled apart and shared between users, but that is still within the navigation and semantic models of “NEWSPAPER”. Similarly, a no-script HTML web page is passive, because once rendered by the browser (possibly with reference to a style sheet) no further change is made beyond possibly scrolling the content.  Note that the ability for a user to change a style sheet in the browser does not make a document dynamic as in practical terms browsers will (or at least should) re-render the complete page, effectively restarting the session with the user.
A document is dynamic if new instances are realized within the document during a single session with a user. An HTML Web page becomes dynamic when it includes scripting. Using onMouseOver=“…” to implement a button rollover for example, makes the page dynamic since what is rendered is modified during presentation. Similarly, some Microsoft Word documents become dynamic if they include interactive macros, or embed interactive content within them. Even such simple actions as rollovers are important to consider, as there may be equivalent behavior to consider when content is adapted to other design spaces, for example button rollovers may become changes in pitch of the avatar’s voice as it reads the “alt text” related to the button.
The version of the Adaptation Layer presented in this section, addresses only passive documents. This model of adaptation describes how instances are selected and applied to a document as a result of defined events. Note that these are pre-defined instances that can, for example, add/change a language, or map content between design spaces, for example switching from the ‘Default User’ to a ‘Low Vision User’.  

Figure  SEQ Figure \* ARABIC 12 - Adaptation Bridges
The Adaptation Layer has bridges to all other layers (except to external content) as shown in  REF _Ref64890676 Figure 12. Instances in the Adaptation Layer map to instances of individual layers i.e. transactions of add/modify/delete applied to a named “instance”. Each “core” layer is composed of multiple instances of elements (the objects described in rectangular boxes in each layer model), further each instance of each element is related by the bridge mapping to a specific instance in the Adaptation Layer. Put simply, all information regarding the default user is mapped to a “Default” instance, all information regarding modification of the “Default” instance to support a “Low vision” user is mapped to the “Default to Low Vision” instance. When an element in one instance refers to an element in another instance, the referred to element exists as a “shadow” of that instance. This is analogous to the “extern” statement in programming languages such as C and C++, and to importing package content in Java.

PROTOTYPE 
A simple, fully functioning prototype, based on a small fragment of Google Maps [15], is presented to illustrate the practical use of the CISNA Model. A screenshot of the relevant part of Google Maps is shown in  REF _Ref64894848 Figure 13.

Figure  SEQ Figure \* ARABIC 13 - Search Menu
The emboldened area of  REF _Ref64894848 Figure 13 may be interpreted in many ways. For this example, it is assumed to represent a top-level navigation menu for Google’s applications, with the “Maps” text representing the current context; even though the menu/text is a sequence of text hyperlinks, the downward facing triangle at the end of the list suggests that it is a menu, and that menu item “more” has a sub-menu. To further shorten the example, only the first three links plus “more” plus the triangle are considered. From this foreshortened menu, two users are considered: a default user, and a low vision user who requires text-to-speech in addition to the default visual representation.
Five text elements exist in the Inventory Layer as Raw Media Elements: “Web”, “Images”, “Maps”, “More” and the triangle character. For the default user, these require formatting as FormattedMediaElements for use with fonts. For the low vision user, we require additional formatting for text-to-speech. 
Both font and text-to-speech representations map through the Inventory & Semantics Bridge to notions representing the site, menus, and menu items. The resulting semantic model is shown in  REF _Ref64811133 Figure 7 above. 
For the prototype, an XML-based notation was developed to express each layer and bridge. A small example fragment of that notation for the Semantics Layer is shown in  REF _Ref67720478 Figure 14. Each layer and bridge is described similarly.

Figure  SEQ Figure \* ARABIC 14 - Semantics Layer as XML
The prototype application was constructed in Java, using the FreeTTS [16] implementation of the Java Speech API [17]. The application provides a menu to select a user profile, and two output windows, one for the rendered “abstract model”, and for the rendered “concrete” model.
The concrete output, shown in  REF _Ref64897407 Figure 15, is the rendered fragment of the Google Maps page. In addition to the apparent visual content, rendering for the low vision user also produces the spoken text “Application Google Maps. Menu Start. Item Web. Item Images. Item More. End Menu.”


Figure  SEQ Figure \* ARABIC 15 - Concrete Model Output
DISCUSSION
This paper has concentrated upon the abstractions within the five layers of the CISNA document model, with only limited discussion of the Development and Runtime Systems. To render this abstract user interface requires an understanding of user capability in order to identify the content of an “instance” of adaptation. It also requires that appropriate interaction modalities be selected when rendering to each design space (visual, sonic, or haptic). Both of these issues are addressed within a broader research project at the University of Teesside that extends the concepts within Nesbitt’s Multi-sensory Design Space [18], with the CISNA document model forming the core. 
By focusing upon the five-layer stack, the result is a model and discussion centered on abstract user interfaces, and within that context CISNA is but one of many approaches to the problem of separating content from presentation. XForms is perhaps the most notable of these in terms of practical implementation on the Web, but all W3C notations: HTML, XHTML, SVG, and SMIL, to a greater or lesser degree also attempt this separation of concerns through the use of a Document Object Model (DOM). 
What makes CISNA unique compared to the existing DOM approach, is the focus on adaptation of content, and a separation of concerns not only between abstract and concrete representation, but between the different subject areas of concern to adaptation, and by implication, to assistive technology and accessibility. The CISNA Model allows discussion and individual adaptation strategies to be applied when amending content, document semantics, document navigation, and ultimately to dynamically adaptive behavior, discussed later in this section. Further, those adaptations are described in terms of simple instances of add/modify/delete for all areas of concern.
This concept of an instance is one of the big steps forward over the traditional DOM model, where changes to the DOM are expressed through scripting. Each of the CISNA layers, and each bridge are expressed as a Shlaer/Mellor Information Model [13], which is a close relative of the relational database, and this allows for the document as a whole to be viewed as a single, and relatively simple, relational database. Once in database form, all of the standard, established, computer science database theory and methods become available to the rendering engine of the user interface.  One of those approaches is the transaction, which allows for lists of changes based on add/modify/delete requests to be processed. It is this underlying transactioning approach that becomes the “Instance” object in the Adaptation Layer.
Instances are also important in terms of accessibility. One of the perennial problems in discussing accessibility is finding ways to efficiently explain the difference between, say, a default user interface, and one adapted to a specific user need. If a user interface is described using the CISNA Model, then the required changes can be expressed precisely in abstract terms, focusing on content, semantics, navigation, and adaptivity as required. This provides one small step on the road to creating a formal language of accessibility. Further, being able to describe the differences to a user interface, gives rise to the possibility of creating a standard measure of accessibility: how much change is required to adapt a user interface for a given set of representative user profiles? If we consider the default and modified models as expressed in XML, as in the prototype example, we have two sequences of XML that we can compare. Comparing such sequences occurs frequently in DNA research, and one of the measures is the Levenshtein-Distance; this approach has also been recently proposed for analyzing web server log files to identify patterns and differences in navigation through hypertext. Measuring accessibility in this way is currently outside of the scope of the CISNA Model research but is clearly a route forward for future work.
The CISNA model as described in this paper, stops at the point of dynamically adaptive systems. The full model splits the Adaptation Layer into Configuration and Adaptivity subsystems. The adaptivity subsystem is shown in  REF _Ref64940722 Figure 16.


Figure  SEQ Figure \* ARABIC 16 - Adaptivity Subsystem
The Adaptivity Subsystem still describes behavior in terms of an Information Model, with the key element, the Action object. An Action expresses procedure (e.g. scripting in a web page) and relates that procedure to the elements and attributes of the individual layers affected by the Action. This gives enough basic information for the designer of assistive technology to understand what content within the layers is transient when considering adapting content, or moving content between design spaces. The question then becomes: can an Action itself be adapted, and can that adaptation be expressed within the CISNA notation? The answer to that appears to be “yes”, and part of the research surrounding the CISNA model treats Actions as Directed Graphs, and since Shlaer/Mellor Information Models express Directed Graphs, Actions may be expressed in the same notation. This is ongoing work.
One noticeable effect of the CISNA Model on the XML descriptions used in the prototype example is the sheer quantity of XML necessary to express even a trivial application, certainly in comparison to the short fragment of HTML that would be needed to express it. Part of this volume of XML is in fact stable content; the underlying semantic model of the ontologies, verbs, and rules of a document are constant and arguably should not be expressed in each and every document that uses them. With this in mind, the XML separates each layer into <model> and <content> fragments (see  REF _Ref67720478 Figure 14) with the expectation that the model would be referred to in a manner analogous to a DTD referenced at the beginning of XML fragments. This reduces XML volume on the trivial example prototype example by almost 50%. The volume of XML in the <content> fragments to some extent reflects the amount of additional information provided in comparison with the equivalent HTML. In the Inventory Layer, it is possible to express different renderings of content; if the CSS styles necessary to describe text-to-speech encoding are added to the HTML, the differences in XML-HTML volume largely disappear. Similar observations are possible for the Semantics and Navigation Layers. 
The CISNA model is a general, conceptual model of a document and as such for it to be correct, it must be capable of embodying all existing documents in addition to any created with the model in mind. If that hypothesis is true, then all existing web pages, for example, must be expressible within the model and not only the simple prototype example presented here. Even a brief consideration of that Google Maps prototype, suggests that doing so without the original author’s input is non-trivial, requires manual input, and is to some extent guesswork based upon experience. How should the line of hyperlinks in the example be interpreted? As a menu? As simple hyperlinked text? What does the non-highlighted “Maps” text represent? What does the closeness of the hyperlinks to the text input field represent? All of which suggests that, in the general case, automated transcoding from HTML source to a structure expressing the CISNA five-layer model will be at best a poor approximation to the original semantic meaning.
CONCLUSION
Experience with the five-layer CISNA Document Model has demonstrated that it is capable of effectively expressing non-adaptive user interfaces, and ongoing work indicates that adaptive environments are also supportable.  The most significant step forward in terms of accessibility is the ability to clearly and simply express what changes are required to adapt a document/user interface to a specific user’s needs, and that list of changes can be supplied as a “patch” to the document. This is analogous to a style sheet in HTML, but rather than simply changing the decoration of the document, it can make substantive changes, adapting/augmenting the content, semantics, and navigation. In this respect, it is a significant step forward for accessible design of user interfaces and electronic documents.
REFERENCES
Halasz, F. and Schwaertz M., The Dexter hypertext reference model, In Communications of the ACM vol 37, issue 2, pp 30-39, ACM, 1994.
Freedom Scientific Inc product catalogue, 2008, http://www.freedomscientific.com
W3C, HTML 5 Working Draft 22 January 2008, http://www.w3.org/TR/html5/
W3C, The XMLHttpRequest Object Working Draft  26 October 2007 http://www.w3.org/TR/XMLHttpRequest/
Google Inc, Google Suggest Website http://www.google.com/webhp?complete=1&hl=en Viewed 18 January 2008
Hardman L. et al, The Amsterdam hypermedia model: adding time and context to the Dexter model, In Communications of the ACM vol 37, issue 2, pp 50-62, ACM, 1994
W3C, Synchronized Multimedia, http://www.w3.org/AudioVideo/ Viewed 18 January 2008
W3C, Synchronized Multimedia, Appendix B http://www.w3.org/AudioVideo/ Viewed 18 January 2008
W3C, XForms specification version 1.0, 2006. http://www.w3.org/TR/2006/REC-xforms-20060314/ 
ISO, ISO Prolog Standard ISO/IEC 13211-1:1995
W3C, Extensible Markup Language (XML), Fourth Edition http://www.w3.org/TR/REC-xml/
Svanaes D. and Verplank W., In search of metaphors for tangible user interfaces, In Proceedings of DARE 2000 on Designing augmented reality environments, ACM 2000.
Mellor S.J. and Balcer M.J., Executable UML: a foundation for model-driven architecture, Addison-Wesley, Reading, MA, 2002.
Widgit Software, The Widgit Symbols Development Project,  http://www.widgit.com/widgitrebus/ Viewed 4 February 2008
Google Inc, Google Maps website, http://maps.google.com/ Viewed 4 February 2008
Sun Microsystems Inc, FreeTTS http://freetts.sourceforge.net/docs/index.php Viewed 4 February 2008
Sun Microsystems Inc, Java Speech API http://java.sun.com/products/java-media/speech/ Viewed 4 February 2008
[18]	Nesbitt K.V., Modeling the Multi-Sensory Design Space, In Australian symposium on Information visualization, - Volume 9 (CRPITS’01), Australian Computer Society, 2001


PAGE  


PAGE  






