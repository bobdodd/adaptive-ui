First Person

Background
The PhD thesis uses two case studies; the first looks at user and context profiling, and the second looks at rendering of content to match a given profile. These observations come out of that second activity.
Tetris
The rendering case study uses the game of Tetris as its example. Tetris of course, was written as a purely visual game, where the player looks down onto the playing surface, observing the position and shape of falling tiles, and the terrain of the “ground” the tile will land upon; the goal being to land the tiles such that complete horizontal lines across the terrain are formed, thus scoring points and causing the complete line(s) to be removed from the playing area.
Audio Metaphors
To construct an audio version of Tetris, I first needed to invent some audio metaphors to use. Those I chose were:
Aside - literally an aside... I whisper the type of the next tile and the content of the hold box into the player's right ear.
Musical sonar. I needed a way to express the quality of the tessellation between the falling tile and the ground.  So, I play a single note for each column of the tile's width in sequence around the user. The tune repeats every couple of seconds, or when the user moves/rotates the tile. The higher the note, the better the fit. It works surprisingly well. Well, once you get the idea. Taodi took a while to understand....
Dancing margins. I needed a way to describe the distance of a falling tile from the edges of the playing grid. This is complicated by the fact that existing fallen tiles can obstruct movement. My solution was to place a sound left and right of the user, and to use the distance in 3D to express the grid distance. Unfortunately, the quality of the 3D audio engine I have on the Vaio is not very good, so  I'm having to make the sound "dance around", forwards and backwards slightly to help the user's ears pick up the locations better. (Note: even this hasn’t worked so well, and the “dance” is now a dance in music, not location. Oddly, this does seem to make the position of the margin clearer). 
Talking scrollbar. This is back to the idea if speaking text from left to right so that the user can tell how far through the text you are. Again, the 3D audio is not great, and you can hear it jump a little. I've scaled the idea back to only 3 locations, so that the sound of the falling tile plays left/middle/right in front of the user, taken with the dancing margins, it helps locate the tile in space.
Direction as direction. I needed a way of describing the orientation of the falling tile. Essentially north, south, east, west. I can just speak the direction, but I'm already whispering in the player's ear, so instead I decided to try animating a sound passing the user in one of 4 directions. Originally it was to be forward/back and left/right combos, but the forward/back doesn't work well with the 3D audio quality (again!) So I tried to rotate the directions by 45 degrees horizontally, making the combos NW/SE and SW/NE and vice versa. Since what is important is orientation, not real direction, it's not a problem. Note: having experimented more with this metaphor, I’ve found that, for the quality of the 3D sound I’m creating, the metaphor does not work too well; I had to have the sound move forwards and backwards in front of the user to gain any accuracy in identifying the location, and the SE, SW sounds are very odd to listen to. Consequently, for now, I am speaking the orientation, but in a separate voice (male rather than female) to that of the tile description. 
Gravity as waterfall. I needed some means of describing the action of falling for the tile, and a measure of how far it is falling. And I needed to do so in a very busy soundscape. My solution was to use the sound of falling water as an ambient sound, rather than a point source, and manipulate the volume and pitch over time, so that the water feels nearer with time. Note: I’ve actually now implemented this as a point source – there seemed to be a qualitative difference between tweening the volume and tweening the location. Of course that may just be me.
Braided audio. Braided audio is a technique that was used as part of the “Audio Hallway” project to allow navigation of large quantities of music tracks. The idea being that the play out of many tracks is spliced together, giving a second or so of one track, then a second of the next and so on, looping back to playing out the first track again.  The user would be presented with one or more braids to select from, with the selected braid then “exploded” to allow a more detailed examination and track selection, potentially to another, more specific braid, and in effect, tunnelling down into an audio database.   What I wanted to adapt the braided audio for was to braid the play-out of the musical elements: the musical sonar and the dancing margins.  Playing out both tunes at the same time, even from radically different locations, is discordant and distracting, so the aim was to serialise them; the first margin, a scan of the sonar, and then the other margin. Since the margins are less important than the sonar when playing Tetris, I also wanted to prioritise the sonar over the margins, say two scans of the sonar for one scan of the margins.  So, I am using braided audio to both share a resource (musical play-out), and to express importance/priority.
Voice
What I find interesting in the created audio metaphors, is the effective change in the voice of the game. Tetris goes from being a 3rd person observational game to a 1st person immersive experience. 
And it wasn’t deliberate. 
The game became immersive because the player became the centre of all interaction modalities. The tile moves relative to the player (and simultaneously, the distance of the margins from the tile are described relatively to the position of the user), gravity ebbs and flows towards the user, and the sonar plays out around the player).
Realizing that I had changed the nature of the game, I looked for alternative, observational, audio metaphors to express my concepts of gravity, tessellation, and relative position, but beyond a screen-reader approach, perhaps using a number of different actors to help identify content, I came up empty. Perhaps it’s in the nature of the sonic design space to be 1st person immersive for anything beyond a simple linear play-out of content?
The thing is: if a 3rd person observational game naturally becomes a 1st person immersive game, what should happen to the classic WIMP interface? Windows, Mac OS, and Linux’s Gnome are all very much 3rd person observational visual interfaces. What is provided by current assistive technology is very much the descriptive, spoken, multiple actors, screen-reader approach. My experience with Tetris tends to suggest that this is an extremely limited set of metaphors, and that there is a much richer set of audio metaphors waiting to be explored. But that of course requires the UI to be described in abstract terms and rendered according to user need; which of course leads straight back to my Carnforth model.
Immersion as the Default Modality
Looking at what happened when Tetris moves from the visual to the sonic design space, made me question how often observational modalities are valid.
Just thinking about some of the assistive technology I see applied, it does seem to be more immersive:
Scanning. The use of scanners to roll through menus of letters of the alphabet, or menus of tasks (open door, close curtains, TV on/off to use the example of Paul’s new system) is more immersive. The user is either visually following the scanner waiting for “their” letter to be highlighted so that they can hit they can hit their jelly-bean switch; or waiting for a spoken menu to reach their choice again to allow selection from a jelly-bean. It becomes immersive as a result of the user having to follow a play-out, and then react within a given time.
Zooming. The whole Zoom-Text approach is immersive, in that the user is placed at a location on the screen, and then navigates around, down, and up through content, and at varying levels of detail. They are no longer looking down on the content, they are looking around within the content.
Page re-ordering. The whole point of re-ordering web pages is to change the user’s perception of importance. For example, moving menus after the main body of content within a page when screen-reading, is to guide the user’s navigation of content. It is perhaps less immersive than zooming, but it is certainly 1st person, with the user being led through the content.
I can’t think of any 1st person modalities becoming 3rd person observational as a result of existing assistive technology, it only seems to go in one direction. The area that it may happen, is with hearing or haptically impaired users. Taking computer games as an example, audio and tactile feedback tends to be used to convey emotion and immediacy to the game. If you have no sense of touch, the feedback in modern games controllers will be missing, and if you are hearing impaired then off-stage action may be missed, as may the movement of other players/game characters. The missing information may be provided visually through additional screen icons, turning 1st person to 3rd in this respect, but with, I’d argue, a resultant loss of emotive content.

Adaptation of Content for Deaf/Hearing Impaired Users
Following on from the previous paragraph, if the change in rendering of content, for example off-stage action games, reduces emotive expression, has any content been lost? Is the adapted interface actually accessible? 
It’s one of those questions that sit on the border of a very arbitrary split between accessibility and usability; but the answer has to be, that if timely information is lost in the adaptation, then the interface cannot be described a wholly accessible. The quality of the underlying abstract description of content becomes paramount. In games at least, content may need to be described in terms of happiness: does the information convey good news or bad?  That requires we have visual metaphors capable of providing that emotive feeling, and a simple addition icon is unlikely to cut it in term of such expression (though an animated one might).


