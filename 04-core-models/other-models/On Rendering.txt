On the Rendering of Content

Reflection
Before starting to consider how to render content, it’s necessary to reflect on the first stage of the modality selection process. 
The most interesting aspect of stage one is, in many ways, how much filtering was possible without considering actual content or its organization into navigable “clumps”. Of all available modalities, we have filtered:
By support for Nouns in the Semantics Layer.
By support for Rules in the Semantics Layer.
By Device and Environment capability.
By User Capability.
So, for any specific device, and any specific environmental conditions, we can identify which interaction modalities are potentially of use to a specific user. That, in itself, is a significant contribution to knowledge:
 It’s the kind of information UI designers need when considering their users. 
It’s the kind of information a project like Chris’s expert system needs to help guide designers.
It means that a big chunk of the filtering process is effectively off-line in that it needs only to be calculated at the beginning of a user’s session, or when ExternalInfluences in the Capacity and Requirements models trigger changes in functionally dependent settings.  That’s important in terms of the processing power needed when rendering content – you don’t want to have to re-filter available modalities every time an application or operating system opens a new window/frame/panel. 
The stability of the filtering over a typical user session also suggests the possibility of an on-line user profile server that can suggest sets of modalities to client applications running locally, again reducing the processing load on mobile devices. 
A second point worth noting is the organization of the Semantics Layer into rules and nouns; and notions and statements. I remember being asked (by Simon Harper as it happens) why I’d taken this approach, and especially the notation I associated with it. The answer I gave at the time was that the notation itself didn’t matter; it was the underlying separation between abstract and concrete that I wanted to get at. The filtering process of stage one makes the reason even clearer: I need that separation of concerns to be able to filter modalities independent of explicit content. And that required first separating semantics from navigation, and then rules from statements. It was that question from Simon Harper that made me think he was a good choice for examiner.
Under the Gooseberry Bush
Given that at some stage we are aiming to render content, where does content actually come from?
 Within the CISNA Model, content resides in the bottom four layers, and is managed by the Adaptation Layer. The raw media physically exists “out in the wild” (if stored media can be described as physically existing) in the Content Layer. The Inventory Layer locates that raw content as RawMediaElements, and offers multiple adapted views of it in the form of FormattedMediaElements. The Semantics Layer expresses the ideas and concepts communicatable between user and computer program, which is also a form of content. The Navigation Layer expresses meaningful organizations of those concepts and ideas and identifies navigable paths between the grouped ideas, and that is also a form of content.
But that still doesn’t describe how the Model gets populated. As with profiling, and the problems with Access for All (which is part of chapter 4 of the thesis), we need to consider workflow in detail.
Workflow Overview
The classic UI design process involves some form of requirements analysis and (a potentially paper) evaluation of prototype designs before beginning implementation of a single “core” rendering of the interface. This “core” interface may be configurable to a greater, or to a lesser, degree through user options and alternate “skins”; those configuration options may require that the designers add additional content. Once delivered to the user, the designers’ only input is to product updates. Any additional content whether it is raw content; semantic organization; or navigational organization, comes from third parties who provide additional “skins”, or alternative input and rendering modalities.
It’s not a perfect split, but it is generally third parties who provide real-time additions of FormattedMediaElements, for example screen reading application that use text-to-speech; and adaptations that use ear-cons to augment tactile feedback.
There are potentially additional sources of raw content that come from users and their communities. Particularly of Web 2.0 style projects come to mind here, that tag images with textual descriptions for use in third-party web page adaptations; I’m not aware of any community project that deals with semantic or navigational organization. Probably, this is because of the simple observation that you can describe what an image looks like, but that doesn’t properly convey semantic meaning which can only come from context of use. There are some images from which semantic meaning may potentially be inferred in some instances (arrows, various road traffic signs, pictures of printers etc) but they are in the minority; this demonstrates some of the limits of Web 2.0 crowd-sourcing as a community solution to accessibility. 
To summarize, there is usually only one complete set of content for many user interfaces. A well-written web page may contain alternate text, long descriptions of media, table descriptions, CSS-3 pronunciation hints etc, but this is still essentially “decoration” of existing semantic and navigational meaning. Returning to the Google Maps menu example, you are unlikely to find both text and images offered as alternatives for use in menus. Alternative text to an image yes; alternative image to a text fragment, no; Rebus symbols, no chance. There are exceptions of course - Word has both image and text centric versions of many menus for example.
The population of content within the CISNA Model is therefore likely to be a relatively sparse one, with only a few minor variations available to a rendering tool. That content, in all three forms (raw, semantic, and navigational) arrives in the form of a default “core” model. In existing traditional expressions of content, separation between the three forms may be significantly obfuscated, with the link between nouns and verbs; and notions and statements effectively broken. The content presented to third party assistive technology by “accessibility layers” provides only the notions and (to some extent, and only by inference) the statements, and provides the information in a pre-defined serial manner. Assistive technology then has to relate the notions to its own equivalent of nouns and verbs to reverse-engineer the semantic and navigational meaning of content in order to adapt it to match user need. This has resulted in research into tools capable of, for example, determining tabular content or proximal content from the accessibility layer stream presented to them (normally using a lot of AI).
Assuming that we can populate the CISNA model, either by the designer explicitly populating it for us, or through AI techniques such as those described above, this does suggest that the ability to turn the RawMediaElements of the Inventory Layer into FormattedMediaElements becomes a critical capability for a rendering tool, if alternative modalities beyond those considered by the designer are to be considered. 
Also suggested by this sparseness, is that unless the designer is encouraged to provide alternative media, and consider alternative modalities, the whole process of rendering to match user need becomes seriously constrained. Or put slightly differently, Accessibility comes from the process of design, not just from the outputs of design; it’s why I like David Sloan’s Tangram Model (from the “Contextual Web Accessibility” paper). Actually it’s also why I think the Tangram Model needs also to consider workflow.
Workflow as Data Flow


The figure above shows one possible abstract model of how a user interface may be developed. It expresses the development process as a Data-Flow Diagram (DFD), with the inputs and outputs of each activity described as data stores. The model I’ve chosen is an iterative approach where iterations of the requirements capture activity are supported by feedback from a prototype implementation. Sid Elms, my course tutor for Computer Technology back in the days of Teesside Poly, used to call it the “suck it and see” model of development. Depending upon the quality of the final prototype, either that last iteration becomes the delivered interface, or it becomes the specification for the formal design. 
The model shows user, device, and environmental capabilities from my Capability Model feeding into the Requirements Capture activity. This implies, rightly, that I have made selection of user base a precursor to the capture of content and interface style. An equivalent DFD for the user base workflow should really exist in chapter 4... 
The Abstract User Interface store is defined as read/write for the Requirements Capture activity. This follows from the iterative approach; presumably the interface is designed with the expression of particular content in mind; but over time, and with experience of individual prototype iterations, those ideas are clarified and adapt to the better appreciation of the content and context of use. Part of the Requirements Capture activity is to take this abstract UI and express it within the CISNA Model. 
The workflow model also shows the user, device, and environmental capabilities being filtered (the three identification activities) to match the scope of each individual prototype. This covers the idea that prototypes tend to express quite narrow use-cases within the overall design, considering specific contexts of use, and specific content. Each prototype adds to the overall understanding of context and user, but an individual prototype may describe only small (and potentially overlapping) fragment of the whole.
Feeding into both the Requirements Capture and the Prototype Design activities is a Modality Meta-data Library. That library expressed the currently supported modalities for the design, for example ways to render lists or menus, or ways to add content to a list. The library is read/write for the Prototype Design activity as potentially new modalities can come out of the prototyping experience. Practical experience suggests that the entirety of the library does not arrive in this fashion, more likely it comes from the GUI libraries used by the developers, for example the Java Swing library.
The key outputs of the Prototype Design activity are the entries in the CISNA Model’s layers.  As iterations of the prototyping exercise complete, and user feedback is evaluated, additional content is added to the CISNA model so that by the final iteration the interface is fully described within CISNA. If the approach to prototyping has been incremental, then the rendered prototype interface will also express the content of CISNA.
The workflow model also shows the CISNA Semantics Layer nouns and verbs being created as part of the Requirements Capture activity. This reflects the idea that if we can choose modalities, then we must also know which nouns and verbs our semantic understanding of the content require.  Again, within the iterative approach considered, the entries within the Semantics Layer will vary, and build over time. They will vary, because as the designers’ understanding of content and context improve, their choice of nouns and verbs to express that understanding will change.
Sitting in the centre of the workflow model is the design language specification. This is the document that describes the look & feel of the rendered interface. It ties together the rules governing layout and grouping of content, the range of approved selection modalities fro the interface, and overall appearance of the interface.  Typically on existing projects, this specification is given in largely narrative form (potentially with some example templates). Construction and use of this specification is central to the content-based stage two of modality selection.
Design Language
Even with sufficient raw media to be able to make intelligent selections of modalities for the user, and to be able to define some navigational groupings, there is still a fundamental problem, and that problem is one of design language.
Take a typical three frame web page: title, main menu, and breadcrumb trail at the top; sub menu at the side; selected content in the centre. How do we know to choose that layout to handle a menu hierarchy? There are certain clues – we must have a menu hierarchy that is at least two levels deep, and at the second level of menu, we (probably) need a title for the content; we also need to know that the first two levels of menu are important in stabilizing the user’s sense of  location within the interface. All of that requires some careful work with rules and statements in the Semantics Layer, and possibly some help in the Navigation Layer to help us create our ViewNodes; and even then, it requires some rule-based intelligence to identify the ideal conditions for using a three frame layout.
This whole process of turning semantic information into appropriate groupings, with appropriate group layouts, is properly the domain of interaction design, which is a branch of industrial design.  In large manufacturing companies (the Nokia’s, HP’s and BMW’s of this world) the process has two discrete stages: first is the development of a design language for a particular product or range of products in terms of example styles and defined organization/layout rules; followed by an application of the specified design rules to particular devices and interface elements. In UI design, that first stage generally results in a look & feel style guide; such guides can be remarkably detailed to the point where individual designers feel the have no real input to the design. That is currently happening at Nokia, and some staff have left as a result of an over-specified look & feel at the mechanical level.
As it stands, the Design Space Model has absolutely no means of expressing look & feel, and consequently, we cannot even identify a particular modality with a look & feel, let alone describe how it should be used within it. One step towards correcting this is the addition of Modality Sets to the model.
Modality Sets
Reflecting on the results of the first stage of filtering by context and pure capability made me question whether there was anything else I could get out of that work before considering filtering by appropriateness of modalities to the given content.
The answer is: yes. The remaining modalities after filtering (hopefully) reflect a broad range of look & feel possibilities, but are not sorted into categories. This level of sorting was something I always thought of as a third and final stage filtering, where we test which look & feels have sufficient modalities left to present a consistent look & feel to the user without having to “borrow” from other categories.  In fact, it seems to make sense to have it built directly into the Design Space Model. At the risk of drowning in ontologies, Elements have a M:M:1 relationship to ModalityOntologies. The relationship is many to many since the same element may be of use to multiple Look & feels, for example many different look & feels may use pop-up menus.

With that relationship in place, it is also possible to suggest to a designer which look & feels in terms of ElementOntologies are available to a user, and which are incomplete (but may still be preferable). These groupings by ontology are my Modality Sets (to go with Capability Sets).

Design Language Sets
Modality sets can only describe what modalities belong to a specific look & feel, they do not describe how such modalities work together. Clearly, there is some additional modelling required.
In terms of existing UI design tools, design language when not given purely as narrative, is generally specified through the use of templates or storyboards; Adobe’s Dreamweaver takes this template approach for example. CSS style sheets also help in this respect, but are more concerned with presentation rules for a given content than directly with semantic or navigational aspects (although CSS2 and CSS3 in small measure do allow some hyperlinks and some content to be defined or referred to).
Within my Shlaer-Mellor modelling approach, design language has its own separate problem domain with bridges to the Design Space Model, and this is covered in a separate note. What is interesting at this stage (essentially half-way through the modality selection method) is the Design Language clearly has an impact upon modality selection, and that like the Modality Sets, seems to have some use before explicit content is considered. It is easy to imagine a Design Language Set being suggested to a UI designer based on user, device and environmental considerations.
Simple substitution based templates, or more complex rule-based ones, still only describe how particular groups of nouns and verbs may work together, they do not explain why or how such templates are either chosen or created. Putting aside the question of how templates are created, which is definitely outside of the scope of my PhD, I do need some means of selection between templates (or template sets?) since modality selection is partly dependent upon it. This is covered in more detail in the following Design Language note.



