Sonic Design Space

The Audio Clip
The Sonic Design Space revolves around the AudioClip. The model chosen is to describe how AudioClips play out on AudioChannels, and how those channels are grouped into AudioStreams.  AudioStreams in turn are associated with specific AudioDevices.
Working backwards, many if not most mobile phones and PCs have at least two AudioDevices, a piezzo-electric buzzer used for simple alerts, and a high fidelity output for music and speech. That high fidelity output may be plain stereo, or increasingly commonly, be also capable 5.1 or 7.1 surround sound. Depe3nding upon the hardware used, a number of audio streams may be played out contemporaneously, mixed by the audio device. Audio streams, in my model at least, contain either 1 or 2 AudioChannels. Typically a stream containing 1 channel represents mono-aural content, a stream containing 2 channels represents some form of stereo content. Playing multi-channel streams through a mono-aural device is undefined, but certainly I have experienced this as play out of the left channel only, rather than a mixed version of the two channels.
AudioClip itself sub-types into three areas: music, synthetic audio, and natural audio.  Synthetic audio covers tones and text-to-speech style audio. Natural audio covers recordings of human speech and other environmental recordings. To some extent this is a fairly arbitrary decomposition, selected by the effect user capability may have on its perception. For example, a person with a poor frequency response, for example impairment caused by listening to loud music, may have difficulty recognizing subtleties of music composition. A second example is the effect that poor high (or low) frequency response may have on the recognition of certain tones. There is also, potentially, a difference in a person’s understanding between a synthetic computer generated voice, and a recording (or play out) of human speech. 
There are three forms of dialogue that are generally associated with audio, and are typically viewed as potential augmentations that may support disabled users, namely: score, script and transcript. The score is interesting. Generally, music play out on TV does not provide a live score in the same routine way that is done for human speech; presumably we don’t think deaf people will gain by that addition. The one occasion where the score (to a limited extent) is given is in terms of song lyrics, which happens on religious broadcasting, and karaoke based programming. Script and transcript are rather more common; the difference between them being that a script is known in advance, a transcript may or may not be known in advance, for example live subtitles on TV news broadcasts.
Dialogue
Whatever the origin of the Dialogue in terms of score, script or transcript; each dialogue is assumed to be associated with a single language, science fiction programs not withstanding. It is more than possible that the same transcript is available in multiple languages. And that of course includes performance based languages, and haptic representations such as Braille. And that, I think is where the presented AudioClip model fails: with disjoint visual, sonic and haptic design spaces, the Dialogues associated with audio tracks cannot be properly represented. Again, like the VisualElement Model, we need to rely on the Augmentation Model within the general Design Space Model to express dialogues. The problem is that the Augmentation Model as it stands does not organize those augmentations; it simple relates individual augmenting Elements to their “root” Elements. This suggests we also need a general means to express containment of Elements at Design Space level. 




Resource Competition and Synchronization
The thing that is most striking in the Sonic Design Space rather more than in the Visual Design Space, is the potential for competition for (audio) resources. Even allowing for unlimited mixing of AudioClips, how useful contemporaneous speech clips, tones, and music may be, is questionable. There are, as it happens, modalities designed to exploit exactly this form of composition: cocktail party effect based modalities and braided audio are two examples, but more generally, audio is ambient background non-speech sounds or music plus, at most, one single foreground ear-con or speech fragment. 
Competition for audio resources can be resolved in one of two ways: selection and rejection of stream requests that overlap the same time window, or enforced time delays based on a priority scheme. 
A related problem is synchronization of audio clips, in general (start together, end together, cross-fade etc.). In hypermedia generally, synchronization, and to a lesser extent competition is resolved by adding some form of time component to the Dexter Model of Hypertext, with the Amsterdam Model, and my CISNA model are two examples of this. This leads to the question: within the model of the Sonic Design Space (and indeed all three spaces) does this competition for resources need to be expressed?
The answer comes down to what the design space models are being used for. In my case, I want to describe which properties of the design space impact upon the selection of modalities for a given user, and that clearly does mean having some understanding of potential conflicts for audio, video and haptic content.
So what does the AudioClip represent? Is it significant information? How timely does that information have to be?  Is it navigational feedback to the user?  Does it provide ambience?  All of which tends to point in the direction of my Capability Model, rather than the Design Space Model. It makes me suspect there are multiple instances of Capability within the Design Space that relate to the Element itself; to the relationships between elements (augmentation, containment, synchronization etc), and presumably in ontological grouping around interaction metaphors (WIMP for example).
Animation
Running directly on from consideration of synchronization, is the question of animation of elements. Clearly visual elements can be animated, but so can sonic ones: we can vary volume over time, fade-in and fade-out for example; we can pan a point source of sound in a stereo environment, or combine that pan with volume control to give (a poor) simulation of animation through a 3D design space; or we can use spatial audio to more accurately describe animation of a point source of sound in 3D, which unlike the pan and volume control can provide y-axis panning. 
Such animation may be constructed by a particular modality, or it may be pre-recorded into the audio clip, which poses the question: what does an AudioClip represent in the design space? Is it that raw unprocessed audio “file”, or is it the processed clip rendered at a specific location? That Elements expressed in the Design Space Model are defined at a Location, suggest that it must be the latter case, but that does not make sense if the Location can vary. 
My modelling solution to this is to characterize Location, and to say that the Location object has a dynamic lifecycle within the rendered design space, which is certainly true of most audio clips (even a microphone gets turned on and off). Visibility of the Location to the user and its location in space can therefore “tween” over time, to use the old Macromedia term. Similarly, visual elements such a radio buttons, check boxes, rollover images, scrolling text (ugh!) and animated elements whose coordinates vary over time also tween. 
It is probably the tweening property of an Element that is of most interest here, as it is a user’s ability to track moving content, and to recognize morphed content (the essence of most tweening) requires investigation before a modality that tweens content can be selected for a specific user.
This leaves me with two more characteristic properties at Design Space level: tweening of the Element itself, and tweening of the Location. So, again, I seem to be heading for a model of Capability Model of Elements, and now a Capability Model of Location.






 
