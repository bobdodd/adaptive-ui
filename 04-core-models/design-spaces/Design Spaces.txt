Design Spaces
My thesis is constructed upon 4 pillars: the Shlaer-Mellor method; Capability modelling; the CISNA abstract model of content; and design spaces. This note covers my abstract (as always) model of a design space.

Nesbitt’s Design Spaces
The concept of the multi-sensory design space comes from Nesbitt, who described three design spaces: visual, sonic, and haptic (which Nesbitt would call ‘tactile’). In Nesbitt’s terms those design spaces are instances of “Information Perceptualization”, characterized by visual, auditory, and haptic data structures. Nesbitt’s model is based on the Card-Mackinlay process model of how raw data is processed into views of information perceivable by the user. Nesbitt expands on this concept to cover sonic and haptic design spaces, and changes some of Card-Mackinlay’s terminology to improve consistency of use across the design spaces. 
In the approach, raw data is characterized into: Nominal, Quantitative, and Ordered. Quantitative data is further divided into: Temporal, Spatial, and Geographical. The characterized raw data is then tabularized in order to describe it in terms its attributes. Each table is in effect an object in a Shlaer-Mellor information model [my description]. Visual, sonic, and haptic structures are then created that augment instance of the objects with “marks” and “graphical properties” to create views of content. Essentially this is the same process that occurs in rendering an abstract user interface, which is what the process most resembles.
Design Spaces in Abstract Terms
The model of accessibility (through adaptation) that I’ve created, builds upon the idea of an abstract user interface that is rendered to best reflect user, device, and environmental capabilities at any given time, within the limitations of the interaction modalities available on the device. Interaction modalities operate within design spaces, and my interest is in which physical characteristics of those design spaces are utilized by each modality. I use this information to select specific modalities based on the user’s physical and cognitive capacity to interact within those design spaces.
I started, right back almost at the beginning of my research with my own Shlaer-Mellor models of the visual, sonic and haptic design spaces. For a while I also considered a cognitive and a learning design space, but in the end, I decided that I was mixing capability and rendering. I needed the design spaces to reflect the perceivable characteristics of a device in order to express the mapping between user capability and the device’s capacity to support the user. If the device was considered “intelligent” such that the user can perceive learning and cognition in the subtle behaviour of the device, then perhaps the two additional design spaces would make a come-back, but that degree of AI is outside of the scope of my work. [That question of additional design spaces when considering AL probably counts as one of Simon Lynch’s sweeties for this chapter].
The abstract model of a Design Space that I have now (shown at the beginning of this note) is the result of analysing those original models, looking for commonality. The reason for this abstraction stems from the goal of that mapping between user capability and device capacity, which relies on describing the characteristics of modalities. Since landmarks can appear in multiple design spaces, a point source of sound in virtual or augmented reality for example, even existing within the same geometry, it seemed reasonable to describe design space in a general sense, rather than using the explicit models of Nesbitt’s multi-sensory design space; it is only at the sub-types of Element that Nesbitt’s design spaces become visible once more. The fact that an Element can exist in multiple design spaces simultaneously explains the apparent multiple inheritance in the model. 
What the multiple inheritance is really showing, and I find is often the case with Shlaer-Mellor models,  is that the Design Space problem domain is probably a client of separate visual, sonic, and haptic domains, with bridges existing between client and services. If this is the case, then the super-types should be removed from the model (and are in my work proper).
Landmarks
A landmark is related to, but different from, Nesbitt’s concept of marks on a spatial substrate: “For visual structures, marks are simply elementary things that are visible in the spatial substrate (Card, Mackinlay et al. 1999). The most elementary types of marks are points(0D), lines(1D), areas(2D) and volumes(3D)” (Nesbitt, section 2.5).  In the Design Space model, a Landmark is simply a notable location within a given Geometry at which an Element may exist. The same Geometry may exist in multiple substrates (although I don’t have an explicit object of “Substrate”). Further, the same element may exist on many devices contemporaneously, and also on the same device contemporaneously, at different Locations. Six practical examples will help explain why this is necessary:
Split screen editing of text. It is the same text but shown in two separate windows.
Dual monitor personal computers that share a common “desktop”, or that have windows that may appear on more than one device, contemporaneously.
Virtual desktops which allow the same window to appear on multiple desktops apparently simultaneously, and may be presented as a thumbnail on the desktop “map”.
A breadcrumb trail on a web page – the current page exists as itself and as an entry on the breadcrumb trail or page history list.
Thumbnail windows in CAD tools that allow a whole image window to be visible whilst the user has zoomed the editing window.
An “Incoming mail” message on a mobile phone that may simultaneously be represented by: a flashing envelope icon; an earcon; and a  vibrating (hapticon? Is that a word?)
In all these cases, it is the same element that is being presented, albeit potentially rendered in alternative forms.
This concept of multiple instances of the same element but rendered differently in different locations in potentially multiple design spaces, is extremely important for adaptation; if you are going to adapt content, you need to understand the linkage between the different entities rendered within the user interface. It also represents reality: mechanical keyboards usually make a noise when a key is pressed, and that may be significant to a user with touch impairment; mobile phone interfaces often provide earcons when lists or menus are navigated representing changes in focus; accessible video provides potentially multiple forms of captioning to accompany the audio track. Modalities are not generally designed exclusively by design space (which is another problem with Access for All and their “alternative-to...” approach to user preference; practically it can make little sense when choosing between modalities, it is what is done with images, text, and sounds that is important.
Significance
In terms of my model of rendering content based upon capability, and a modality/metaphor set, my main interest in the modality/metaphor set is in how meaning is applied to elements contained in the design space. This interest appears as Significance in the Design Space Model. Element of the design space are characterized by the Element Properties, such as height, area, colour, audio track name etc. And each element has some understandable significance to the user when rendered. For example, colour may represent importance, height may represent progression (think scrollbars), area may express distance from the user (think Apple’s Time Machine), a music track may represent discrimination between temporal events (think different ringtones for entries in a mobile phone phonebook). 
Elements themselves appear to have no significance until they are rendered/assigned to a device. For example on a folding phone with a touch screen (or soft keys) and an external mini-screen that is active when the phone is closed, an element representing a new mail message may be an icon and an ear-con when the phone is closed; or an active button on a touch screen when the phone is open, with its active state representing the fact that there is a new message to view. So the element has different significance depending upon where it is rendered. A simpler example of rendering significance is a window expressed on a virtual desktop. The window signifies active content that can take focus within the main desktop(s), but it also signifies its own location within a virtual desktop’s thumbnail view. Essentially it is the same element, but rendered and expressing different significance in each case.
Rendered elements exist at some nominal location within the geometry of the device to which they have been assigned. Such a location may signify many different things. It may be an arbitrary point in space; it may be a layer in a window; it may be a channel in an audio stream. Further, the coordinates of the element, or the z-order of the layer, or the channel number in the audio stream, may represent specific information to the user: the active editable layer in Photoshop, the (x ,y) coordinates of a chair in a virtual world, the left audio track during stereo play-out of a sound. 
SignifierType has counterparts in the Nouns defined within the Semantics Layer of the CISNA model. So a SignifierType could be, for example, a Menu or a Menu Item, in the Google Maps example in the CISNA paper.  Similarly, SignifierType may also have counterparts in the ViewNodes of the Navigation Layer.  That is to say, there are bridges to both the Semantics and Navigation Layers of the CISNA Model.
Properties
Not only does the design space have counterparts/bridges to the CISNA model, it also has counterparts/bridges to the Capacity Model. Specifically, an element’s Properties, such as height, area, colour, and audio track name, have counterparts in the capabilities of the device. For example, an element that requires colour depends on the colour capabilities for the associated modality to be appropriate to the device; similarly, the element has counterparts to the colour capabilities of the user, since the modality is only of use in expressing information if the required colour capability of the user matches the user’s capacity to perceive it. 
This means that one of the tests of appropriateness of an interaction modality is if all of the Elements potentially rendered as part of that modality have their counterparts in the Navigation or Semantics Layers of the CISNA Model, and also have their counterparts in the device, user, and though not mentioned here, the environmental Capacity Models. There are other selection criteria, for example ensuring that a consistent and complementary set of modalities are chosen where possible, with modalities that are contextually appropriate (think cinemas, libraries, cars etc.), but the first cut is the existence of those counterparts in the bridges.
Populating the Design Space Model
Having constructed the Design Space Model as my virtual “substrate” to use Nesbitt’s term, there remains the question of what exactly it is populated with. The answer appears to be two quite different entities, let’s name them the abstract, and the concrete, populations.
The concrete population is the most obvious. A rendered user interface of elements clearly populates the Design Space Model. Taking the menu example from the CISNA paper again, A menu may be rendered visually as a rectangle containing a list of items expressed as rows of text from top to bottom within the rectangle; the colour of the menu items may depend upon whether the user has selected the item, or is hovering over it with a mouse for example. Depending on the graphics libraries in use, this might be rendered as a coloured rectangle in one layer, and a set of left-aligned text elements in a layer with a higher Z order. The name of the menu may appear as text at the top of the rectangle, perhaps in a different font face and size. There would be one element for the rectangle, one for the menu name, and one for each menu item. Potentially there may be ear-cons that play out when the user navigates then menu, perhaps clicking as a mouse moves across the boundary between two menu items; this would also be an element.
The abstract population is a little different: if we are trying to select appropriate modalities for the user, do we really need to know the exact rendering of a menu? I would suggest we don’t. What we do need to know is that the modality uses colour to group content; colour to identify navigational state; and proximate positioning of text vertically to infer order (“Menu contains MenuItem”, “MenuItem follows MenuItem”, and “MenuItem isNear MenuItem”, are the relevant Rules from the Semantics Layer of the CISNA model). So in the abstract case, it seems to be the Rules we are interested in, not so much the Nouns and ViewNodes that the concrete population is reflecting. And this suggests there is another bridge not so far considered between the Rules in the Navigation Layer of the CISNA Model, and Element Properties once rendered in the Design Space Model; such a bridge is non-trivial in that it is an example of N-ary relationship between property, rendered element, and rule.


