Haptic Design Space

The Haptic Clip
The Haptic Design Space is by far the least finished of the three design spaces, largely because I’ve never really had the hardware to gain experience with the design space, but it’s not too far out. 
The model of haptics I have chosen treats haptic communication as streamed, possibly encoded content. That seems reasonable for the encoded “bit” streams we might associate with Morse Code and Braille’ “bit” is in quotes as, with Braille for example, the user may detect a number of parallel bit streams and interpret them as encoded “words”. Similarly, keyboards can usually handle multiple key-presses simultaneously, and it is quite reasonable to expect at least some users to be capable of pressing multiple “jelly bean” buttons simultaneously (if with some lead-in and lead-out errors). Lead-in and lead-out are not explicitly modelled as I’ve considered them as properties of the encoding and decoding strategies.
Inspiration
The inspiration for this model of haptics is relay logic (now that dates me...) where discrete parallel inputs are encoded/decoded/conditioned and the results used to drive SR Flip-Flops (Set/Reset which is all a relay is capable of offering). The discrete outputs of the Flip-Flop feed other Flip-Flops and provide outputs that (generally) drive electrical equipment and lighting displays. As a technology it’s very old to put it mildly, with more modern versions providing time delay elements to handle lead-in timeouts. I was using relays in the 1970’s for my O-level Engineering project to control a model railway (my A-level project used TTL as an alternative...).  I also used it more recently(ish) (i.e. 1980’s) for process control applications at Cadbury’s to make Crème Eggs, with physical relays replaced with a Programmable Logic Controller (PLC). 
Kinaesthetics
It’s the PLC view I think of when considering the more complex encoding and decoding strategies that go beyond mere tactile sensing and reach into kinaesthetics.  
For example, it is possible for a user to discriminate between locations in a design space by the physical position of their limbs (it’s why you can touch your nose with your finger, even with you eyes closed). Similarly, there is a qualitative difference between, say, touching the back of your ear, and folding it flat, and it’s not (at least on me) to do with the pressure applied.  This leads into simple eye tracking, and into gesture recognition for input where it is sequenced of movements of specific points on the body that need to be decoded, and specific mechanical feedback patterns that are used to communicate directly with the user (computers can use gestures too e.g. the WiiMote). 
The kind of conditioning and encoding/decoding involved in such sensing and communication is exactly what is found in process control applications.
This particular view of haptics also feeds nicely into brain-body interfacing, where the connection between machine and human is the nerve endings of the Central Nervous System, or pattern recognition of electrical behaviour in the user’s brain.
Locations
There are three objects in the model that are related to location: HapticSource, HapticStream, and HapticChannel. All three locations are related to a Device. In reality the location of haptic communication is the user, and it is certainly arguable that User should figure as an object in this case. I’ve chosen not to because my interest is in the devices that communicate with the user and the properties they embody rather than the biological element to which the device interfaces. 
I’m interested in how hard you have to press a button; how long you have to hold it; how precise the user’s movement must be for a gesture to be recognized. Essentially, it is those aspects of the haptic clip that are affected by user capability that need recording.
The location that is most questionable is that of the Haptic Stream. It may well be that conceptually the stream has a location, for example the keyboard, and the placement of the keyboard can affect a user’s capacity to interact, but that is also covered more accurately by the location of the HapticSource and HapticChannel. That suggests to me that the job of the Stream is an organizational one rather than a grouping to a location. It’s in the model, but I do wonder...
Given that I have three locations, it does raise the question of the relationship between Location in the Design Space Model and those in the Haptic Design Space Model. I think the problem is that it is not HapticClip that counterparts to Element but rather, HapticSource and HapticChannel are counterparts of ElementOnDevice.  So, there are three mappings on the bridge between Design Space and Haptic Design Space.
By that argument, VisualElement and AudioClip are also the counterparts of ElementOnDevice, not Element. 



